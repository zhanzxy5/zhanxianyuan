---
title: "Universal Actions for Enhanced Embodied Foundation Models"
collection: publications_preprint
permalink: /publication/2025-uniAct
excerpt: "Preprint, under review."
date: 2025-1-20
venue: 'Preprint, under review.'
paperurl: ''
citation: 'Zheng, J., Li, J., Liu, D., Zheng, Y., Wang, Z., Ou, Z., Liu, Y., Liu, J., Zheng, Y., <b>Zhan, X.</b> Universal Actions for Enhanced Embodied Foundation Models. <i>arXiv:2501.10105</i>.'
---

Abstract
---
Training on diverse, internet-scale data is a key factor in the success of recent large foundation models. Yet, using the same recipe for building embodied agents has faced noticeable difficulties. Despite the availability of many crowd-sourced embodied datasets, their action spaces often exhibit significant heterogeneity due to distinct physical embodiment and control interfaces for different robots, causing substantial challenges in developing embodied foundation models using cross-embodiment data. In this paper, we introduce UniAct, a new embodied foundation modeling framework operating in the Universal Action Space. Our learned universal actions capture the generic behaviors across diverse robots by exploiting their shared structural features, and enable enhanced cross-domain data utilization and cross-embodiment generalizations by eliminating the notorious heterogeneity. Moreover, the universal actions can be efficiently translated back to heterogeneous actionable commands by simply adding embodiment-specific details, from which fast adaptation to new robots becomes simple and straightforward. Our 0.5B instantiation of UniAct outperforms 14X larger SOTA embodied foundation models in extensive evaluations on various real-world and simulation robots, showcasing exceptional cross-embodiment control and adaptation capability, highlighting the crucial benefit of adopting universal actions.


Other information
---
* [Project page](https://2toinf.github.io/UniAct/)
* [Paper](https://arxiv.org/pdf/2501.10105)
