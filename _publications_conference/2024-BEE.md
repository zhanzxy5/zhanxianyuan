---
title: "Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic"
collection: publications_conference
permalink: /publication/2024-BEE
excerpt: "41st International Conference on Machine Learning (ICML 2024)."
date: 2024-5-2
venue: '41st International Conference on Machine Learning (ICML 2024).'
paperurl: ''
citation: 'Ji, T., Luo, Y., Sun, F., <b>Zhan, X.</b>, Zhang, J., Xu, H. Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. In the <i>41st International Conference on Machine Learning (ICML 2024)</i>.'
---

Abstract
---
Learning high-quality value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works primarily focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that values are often underestimated in the latter stage of the RL training process, potentially hindering policy learning and reducing sample efficiency. We find that such a long-neglected phenomenon is often related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates value using both historical best-performing actions and the current policy. Based on BEE, the resulting practical algorithm BAC outperforms state-of-the-art methods in over 50 continuous control tasks and achieves strong performance in failure-prone scenarios and real-world robot tasks. Benchmark results and videos are available at https://beeauthors.github.io.

Other information
---
* [Paper](https://openreview.net/pdf/8ae24c4cb660d3ec4cf3df2602d7dee1f356480a.pdf)