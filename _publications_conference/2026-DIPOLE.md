---
title: "Dichotomous Diffusion Policy Optimization"
collection: publications_conference
permalink: /publication/2026-DIPOLE
excerpt: "14th International Conference on Learning Representations (ICLR 2026)."
date: 2026-1-26
venue: '14th International Conference on Learning Representations (ICLR 2026).'
paperurl: ''
citation: 'Liang, R., Zheng, Y., Zheng, K., Tan, T., Li, J., Mao, L., Wang, Z., Chen, G., Ye, H., Liu, J., Wang, J., <b>Zhan, X.</b> Dichotomous Diffusion Policy Optimization. In the <i>14th International Conference on Learning Representations (ICLR 2026)</i>.'
---

Abstract
---
Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.


Other information
---
* [Paper](https://openreview.net/pdf?id=R8y089OGoo)