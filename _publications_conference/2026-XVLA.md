---
title: "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model"
collection: publications_conference
permalink: /publication/2026-XVLA
excerpt: "14th International Conference on Learning Representations (ICLR 2026)."
date: 2026-1-26
venue: '14th International Conference on Learning Representations (ICLR 2026).'
paperurl: ''
citation: 'Zheng, J., Li, J., Wang, Z., Liu, D., Kang, X., Feng, Y., Zheng, Y., Zou, J., Chen, Y., Zeng, J., Zhang, Y. Q., Pang, J., Liu, J., Wang, T., <b>Zhan, X.</b> X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model. In the <i>14th International Conference on Learning Representations (ICLR 2026)</i>.'
---

Abstract
---
Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: [https://thu-air-dream.github.io/X-VLA/](https://thu-air-dream.github.io/X-VLA/).


Other information
---
* X-VLA has won <b>First Place</b> in the [AGIBOT World Challenge (Manipulation track) @ IROS 2025](https://agibot-world.com/challenge)!
* [Project Page](https://github.com/2toinf/X-VLA)
* [Paper](https://openreview.net/pdf?id=kt51kZH4aG)